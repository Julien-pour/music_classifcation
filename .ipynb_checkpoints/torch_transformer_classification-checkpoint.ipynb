{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:26:28.730085Z",
     "start_time": "2021-04-10T15:26:22.626970Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lejuj\\Anaconda3\\envs\\torchaudio\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, LabelBinarizer, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "#from pydub import AudioSegment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "from torchaudio import transforms\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "from pytorch_pretrained_vit import ViT\n",
    "from pytorch_pretrained_vit.configs import PRETRAINED_MODELS\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import * \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:26:28.867180Z",
     "start_time": "2021-04-10T15:26:28.742042Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 10 17:26:28 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 452.06       Driver Version: 452.06       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   52C    P8     8W /  N/A |    658MiB /  6144MiB |     23%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1720    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      6132    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     10100    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     11456    C+G   ...es.TextInput.InputApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11484    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12136    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\n",
      "|    0   N/A  N/A     12904    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13804    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     14168    C+G   ...s (x86)\\Origin\\Origin.exe    N/A      |\n",
      "|    0   N/A  N/A     14632    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     16272    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     17288    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# panda time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:26:31.768141Z",
     "start_time": "2021-04-10T15:26:28.878140Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('data/fma_metadata/tracks.csv', index_col=0, header=[0, 1])\n",
    "subset=tracks.index[tracks['set', 'subset'] == 'small']\n",
    "tracks=tracks.loc[subset]\n",
    "tracks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset (train, test) + preprocess mono/multi classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:26:31.872121Z",
     "start_time": "2021-04-10T15:26:31.829089Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 training examples, 800 validation examples, 1600 testing examples\n",
      "Top genres (8): ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock']\n",
      "indice data corrupted = 3530\n",
      "indice data corrupted = 3896\n",
      "indice data corrupted = 5609\n"
     ]
    }
   ],
   "source": [
    "# train => track_id train \n",
    "# data => track_id + label\n",
    "train_index = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val_index = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test_index = tracks.index[(tracks['set', 'split'] == 'test' )| (tracks['set', 'split']=='validation') ]\n",
    "print('{} training examples, {} validation examples, {} testing examples'.format(*map(len, [train_index, val_index, test_index])))\n",
    "genres = list(LabelEncoder().fit(tracks['track', 'genre_top']).classes_)\n",
    "print('Top genres ({}): {}'.format(len(genres), genres))\n",
    "data=tracks['track', 'genre_top']\n",
    "# data,train_index[0], data.loc[train_index[0]] # (2, 'Hip-Hop')\n",
    "# pour multi classifier\n",
    "multi_classifier=False\n",
    "if multi_classifier:\n",
    "    enc = MultiLabelBinarizer()\n",
    "    labels = tracks['track', 'genres_all']\n",
    "    test_multi_index = tracks.index[(tracks['set', 'split'] == 'test' ) ]\n",
    "\n",
    "    # Split in training, validation and testing sets.\n",
    "    y_train = enc.fit_transform(labels[train_index])\n",
    "    y_val = enc.transform(labels[val_index])\n",
    "    y_test = enc.transform(labels[test_index])\n",
    "    \n",
    "# /!\\ data corrupted delete them : \"099134.mp3\" \"108925.mp3\" \"133297.mp3\" use cell below    \n",
    "flag=len(train_index)\n",
    "for i in range(len(train_index)):\n",
    "    if flag==i:\n",
    "        break\n",
    "    if str(train_index[i])==\"99134\" or str(train_index[i])==\"108925\" or str(train_index[i])==\"133297\":\n",
    "        print(\"indice data corrupted =\",i)\n",
    "        train_index=train_index.delete(i)\n",
    "        if multi_classifier:\n",
    "            y_train=np.delete(y_train, i,axis=0)\n",
    "\n",
    "        flag=flag-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:35:43.630461Z",
     "start_time": "2021-03-27T09:35:43.626599Z"
    }
   },
   "source": [
    "## (use one time) to convert all mp3 to wav (speed load mp3 << speed load wav) with downsample from 44100 to 22050 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:27:52.750234Z",
     "start_time": "2021-03-27T09:27:52.284935Z"
    },
    "scrolled": false
   },
   "source": [
    "#!mkdir \"data_proces/fma_small/\"\n",
    "path1=\"data/fma_small/\"\n",
    "path_stockage=\"data_proces/fma_small/\"\n",
    "count=0\n",
    "for root, dirs, files in os.walk(path1):\n",
    "    for name in files:\n",
    "        if name.endswith(\".mp3\"):\n",
    "            if not(path.exists(path_stockage+name[:-4]+'.wav')):\n",
    "                try:\n",
    "                    waveform, sample_rate=librosa.load(root+\"/\"+name,duration=15,sr=22050)\n",
    "                    sf.write(path_stockage+name[:-4]+'.wav', waveform, sample_rate)\n",
    "                    count+=1\n",
    "                except:\n",
    "                    print(\"audio :\"+root+\"/\"+name+\"  is corrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer\n",
    "https://colab.research.google.com/drive/1muZ4QFgVfwALgqmrfOkp7trAvqDemckO?usp=sharing#scrollTo=sb13T_M0yFb1\n",
    "https://colab.research.google.com/github/ra1ph2/Vision-Transformer/blob/main/VisionTransformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:26:31.933091Z",
     "start_time": "2021-04-10T15:26:31.931091Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pytorch-pretrained-vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T16:20:03.818910Z",
     "start_time": "2021-04-10T16:20:03.796932Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import ViT\n",
    "\n",
    "class Perso_ViT(nn.Module):\n",
    "    def __init__(self,name_model=\"B_16_imagenet1k\",PRETRAINED_MODELS=PRETRAINED_MODELS,Number_block=None):\n",
    "        super(Perso_ViT, self).__init__()\n",
    "        self.name_model=name_model\n",
    "        self.Number_block=Number_block\n",
    "        self.best_model=40 #a modif\n",
    "        if Number_block:\n",
    "            PRETRAINED_MODELS[name_model]['config']['num_layers']=Number_block\n",
    "        dim_out=PRETRAINED_MODELS[name_model][\"config\"][\"dim\"]\n",
    "        self.model= ViT(\"B_16_imagenet1k\", pretrained=True)\n",
    "        self.model.fc=nn.Identity()\n",
    "        self.last_layer=nn.Sequential(nn.Linear(dim_out,1024),nn.ReLU(),nn.Linear(1024,512),nn.ReLU(),nn.Linear(512,8))\n",
    "        self.model_freeze=True\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.Train_loss=[]\n",
    "        self.Test_loss=[]\n",
    "        self.Train_correct=[]\n",
    "        self.Test_correct=[]\n",
    "        \n",
    "    def freeze(self,all_layer=True):\n",
    "        if all_layer:\n",
    "            self.model_freeze=True\n",
    "        else:\n",
    "            self.model_freeze=False\n",
    "            for param in self.model.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "            # freeze or not the first conv\n",
    "            for param in self.model.patch_embedding.parameters(): #activate training on first conv\n",
    "                param.requires_grad = True\n",
    "            for param in self.model.fc.parameters():\n",
    "                param.require_grad = True\n",
    "\n",
    "        \n",
    "    def unfreeze(self):\n",
    "        self.model_freeze=False\n",
    "#         for param in self.model.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "    def fit(self,epochs,optimizer,device,trainloader,all_spectro,max_lr=1e-5,use_amp=False,grad_clip=0.2):\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, steps_per_epoch=len(trainloader), epochs=epochs)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss,correct_train=train(self.model, device, trainloader, self.criterion, optimizer, epoch,scheduler,all_spectro,transform_train_db,use_amp,grad_clip)\n",
    "            test_loss,correct_test=test(self.model, device, testloader,self.criterion,all_spectro,transform_test_db)\n",
    "            self.Train_loss.append(train_loss)\n",
    "            self.Test_loss.append(test_loss)\n",
    "            self.Train_correct.append(correct_train)\n",
    "            self.Test_correct.append(correct_test)\n",
    "            print(optimizer.param_groups[0]['lr'])\n",
    "            if correct_test>self.best_model:\n",
    "                self.best_model=correct_test\n",
    "                if self.Number_block:\n",
    "                    path=\"model_save/ViT_\"+self.name_model+str(self.Number_block)+\"_spec.pth\"\n",
    "                else:\n",
    "                    path=\"model_save/ViT_\"+self.name_model+\"_spec.pth\"\n",
    "                torch.save(model.state_dict(), path)\n",
    "        plt.plot(self.Train_loss)\n",
    "        plt.plot(self.Test_loss)\n",
    "        plt.legend([\"train\",\"test\"])\n",
    "        plt.title(\"CrossEntropyLoss \"+self.name_model)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.savefig('loss_'+self.name_model+' _spectro_all_spectr.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(self.Train_correct)\n",
    "        plt.plot(self.Test_correct)\n",
    "        plt.legend([\"train\",\"test\"])\n",
    "        plt.title(self.name_model+ \" accuracy % , max acc = \"+str(max(Test_correct)))\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.savefig('accuracy_ '+self.name_model+' _spectro_all_spectr.png')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    def forward(self, x):\n",
    "        if self.model_freeze:\n",
    "            with torch.no_grad():\n",
    "                x=self.model(x)\n",
    "        else:\n",
    "            x=self.model(x)\n",
    "        x=self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T16:20:35.047961Z",
     "start_time": "2021-04-10T16:20:34.196503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys when loading pretrained weights: []\n",
      "Unexpected keys when loading pretrained weights: ['transformer.blocks.6.attn.proj_q.weight', 'transformer.blocks.6.attn.proj_q.bias', 'transformer.blocks.6.attn.proj_k.weight', 'transformer.blocks.6.attn.proj_k.bias', 'transformer.blocks.6.attn.proj_v.weight', 'transformer.blocks.6.attn.proj_v.bias', 'transformer.blocks.6.proj.weight', 'transformer.blocks.6.proj.bias', 'transformer.blocks.6.norm1.weight', 'transformer.blocks.6.norm1.bias', 'transformer.blocks.6.pwff.fc1.weight', 'transformer.blocks.6.pwff.fc1.bias', 'transformer.blocks.6.pwff.fc2.weight', 'transformer.blocks.6.pwff.fc2.bias', 'transformer.blocks.6.norm2.weight', 'transformer.blocks.6.norm2.bias', 'transformer.blocks.7.attn.proj_q.weight', 'transformer.blocks.7.attn.proj_q.bias', 'transformer.blocks.7.attn.proj_k.weight', 'transformer.blocks.7.attn.proj_k.bias', 'transformer.blocks.7.attn.proj_v.weight', 'transformer.blocks.7.attn.proj_v.bias', 'transformer.blocks.7.proj.weight', 'transformer.blocks.7.proj.bias', 'transformer.blocks.7.norm1.weight', 'transformer.blocks.7.norm1.bias', 'transformer.blocks.7.pwff.fc1.weight', 'transformer.blocks.7.pwff.fc1.bias', 'transformer.blocks.7.pwff.fc2.weight', 'transformer.blocks.7.pwff.fc2.bias', 'transformer.blocks.7.norm2.weight', 'transformer.blocks.7.norm2.bias', 'transformer.blocks.8.attn.proj_q.weight', 'transformer.blocks.8.attn.proj_q.bias', 'transformer.blocks.8.attn.proj_k.weight', 'transformer.blocks.8.attn.proj_k.bias', 'transformer.blocks.8.attn.proj_v.weight', 'transformer.blocks.8.attn.proj_v.bias', 'transformer.blocks.8.proj.weight', 'transformer.blocks.8.proj.bias', 'transformer.blocks.8.norm1.weight', 'transformer.blocks.8.norm1.bias', 'transformer.blocks.8.pwff.fc1.weight', 'transformer.blocks.8.pwff.fc1.bias', 'transformer.blocks.8.pwff.fc2.weight', 'transformer.blocks.8.pwff.fc2.bias', 'transformer.blocks.8.norm2.weight', 'transformer.blocks.8.norm2.bias', 'transformer.blocks.9.attn.proj_q.weight', 'transformer.blocks.9.attn.proj_q.bias', 'transformer.blocks.9.attn.proj_k.weight', 'transformer.blocks.9.attn.proj_k.bias', 'transformer.blocks.9.attn.proj_v.weight', 'transformer.blocks.9.attn.proj_v.bias', 'transformer.blocks.9.proj.weight', 'transformer.blocks.9.proj.bias', 'transformer.blocks.9.norm1.weight', 'transformer.blocks.9.norm1.bias', 'transformer.blocks.9.pwff.fc1.weight', 'transformer.blocks.9.pwff.fc1.bias', 'transformer.blocks.9.pwff.fc2.weight', 'transformer.blocks.9.pwff.fc2.bias', 'transformer.blocks.9.norm2.weight', 'transformer.blocks.9.norm2.bias', 'transformer.blocks.10.attn.proj_q.weight', 'transformer.blocks.10.attn.proj_q.bias', 'transformer.blocks.10.attn.proj_k.weight', 'transformer.blocks.10.attn.proj_k.bias', 'transformer.blocks.10.attn.proj_v.weight', 'transformer.blocks.10.attn.proj_v.bias', 'transformer.blocks.10.proj.weight', 'transformer.blocks.10.proj.bias', 'transformer.blocks.10.norm1.weight', 'transformer.blocks.10.norm1.bias', 'transformer.blocks.10.pwff.fc1.weight', 'transformer.blocks.10.pwff.fc1.bias', 'transformer.blocks.10.pwff.fc2.weight', 'transformer.blocks.10.pwff.fc2.bias', 'transformer.blocks.10.norm2.weight', 'transformer.blocks.10.norm2.bias', 'transformer.blocks.11.attn.proj_q.weight', 'transformer.blocks.11.attn.proj_q.bias', 'transformer.blocks.11.attn.proj_k.weight', 'transformer.blocks.11.attn.proj_k.bias', 'transformer.blocks.11.attn.proj_v.weight', 'transformer.blocks.11.attn.proj_v.bias', 'transformer.blocks.11.proj.weight', 'transformer.blocks.11.proj.bias', 'transformer.blocks.11.norm1.weight', 'transformer.blocks.11.norm1.bias', 'transformer.blocks.11.pwff.fc1.weight', 'transformer.blocks.11.pwff.fc1.bias', 'transformer.blocks.11.pwff.fc2.weight', 'transformer.blocks.11.pwff.fc2.bias', 'transformer.blocks.11.norm2.weight', 'transformer.blocks.11.norm2.bias']\n"
     ]
    }
   ],
   "source": [
    "model=Perso_ViT(Number_block=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-10T16:21:05.403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lejuj\\Anaconda3\\envs\\torchaudio\\lib\\site-packages\\torchaudio\\functional\\functional.py:358: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (512) may be set too high. Or, the value for `n_freqs` (1025) may be set too low.\n",
      "  \"At least one mel filterbank has all zero values. \"\n",
      "C:\\Users\\lejuj\\Anaconda3\\envs\\torchaudio\\lib\\site-packages\\torchaudio\\functional\\functional.py:358: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (384) may be set too high. Or, the value for `n_freqs` (1025) may be set too low.\n",
      "  \"At least one mel filterbank has all zero values. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40049cadfea4cbeb2e2cd7c8faeaca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "amp_db=transforms.AmplitudeToDB().to(device)\n",
    "bs=5\n",
    "normalize_spec_db, all_spectro, trainloader, testloader=init_param(model,train_index,test_index,data,bs,label_dic)\n",
    "\n",
    "transform_train_db = T.Compose([amp_db, normalize_spec_db,\n",
    "         # transforms.TimeStretch(fixed_rate =1.2),\n",
    "        #transforms.TimeMasking(time_mask_param=20),\n",
    "        #transforms.FrequencyMasking(freq_mask_param=30),\n",
    "        #T.RandomHorizontalFlip(),\n",
    "        #T.Resize((image_size,image_size))\n",
    "         ])\n",
    "\n",
    "transform_test_db = T.Compose([amp_db,           \n",
    "        normalize_spec_db])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weight_decay = 1e-6\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0)\n",
    "\n",
    "\n",
    "epochs=10\n",
    "\n",
    "model.fit(epochs,optimizer,device,trainloader,all_spectro,max_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:34:21.372646Z",
     "start_time": "2021-04-10T15:34:21.358582Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:34:23.690880Z",
     "start_time": "2021-04-10T15:34:23.685880Z"
    }
   },
   "outputs": [],
   "source": [
    "amp_db=transforms.AmplitudeToDB().to(device)\n",
    "# index = 1\n",
    "bs=2\n",
    "normalize_spec_db, all_spectro, trainloader, testloader=init_param(model,train_index,test_index,data,bs,label_dic)\n",
    "\n",
    "\n",
    "transform_train_db = T.Compose([amp_db, normalize_spec_db,\n",
    "         # transforms.TimeStretch(fixed_rate =1.2),\n",
    "        #transforms.TimeMasking(time_mask_param=20),\n",
    "        #transforms.FrequencyMasking(freq_mask_param=30),\n",
    "        #T.RandomHorizontalFlip(),\n",
    "        #T.Resize((image_size,image_size))\n",
    "         ])\n",
    "transform_test_db = T.Compose([amp_db,           \n",
    "        normalize_spec_db])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "grad_clip = 0.2\n",
    "weight_decay = 1e-6\n",
    "#post_transf=nn.Sequential(nn.Linear(21843,512),nn.ReLU(),nn.Linear(512,8)).to(device)\n",
    "# TRY\n",
    "# post_transf=nn.Sequential(nn.Linear(768,512),nn.ReLU(),nn.Linear(512,8)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0)\n",
    "epochs=20\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-10, steps_per_epoch=len(trainloader), epochs=epochs)\n",
    "\n",
    "Train_loss=[]\n",
    "Test_loss=[]\n",
    "Train_correct=[]\n",
    "Test_correct=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T15:34:36.267622Z",
     "start_time": "2021-04-10T15:34:23.966063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45b0c9b6c06474fb04797429d47475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0837069c0e9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorrect_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_transf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_spectro\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform_train_db\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_train_db\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-3a97f10d6024>\u001b[0m in \u001b[0;36mtrain_transf\u001b[1;34m(model, device, trainloader, criterion, optimizer, epoch, scheduler, all_spectro, transform_train_db, use_amp, grad_clip)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;31m#         loss.backward()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#         optimizer.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch=1\n",
    "train_loss,correct_train=train_transf(model, device, trainloader, criterion, optimizer, epoch,scheduler,all_spectro,transform_train_db=transform_train_db,grad_clip=grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T09:21:36.154694Z",
     "start_time": "2021-04-04T09:21:36.150960Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pretrained on ImageNet-21k and fine tune on ImageNet-1k\n",
    "model_name = 'B_16_imagenet1k' #86859496\n",
    "model_name = \"B_32_imagenet1k\" #88297192\n",
    "model_name = \"B_32_imagenet1k\" #88297192\n",
    "model_name =\"L_16_imagenet1k\" #304 715 752\n",
    "model_name =\"L_32_imagenet1k\" #306 632 680\n",
    "# just Pretrained on ImageNet-21k\n",
    "model_name =\"L_32\" #327 899 475\n",
    "model_name =\"B_16\" #102 595 923\n",
    "model_name =\"B_32\" #104 252 499\n",
    "model_name =\"L_16\" # pretraine param not avaiable\n",
    "model_name =\"L_32\" #327 899 475\n",
    "# count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T10:56:25.320390Z",
     "start_time": "2021-04-04T10:56:16.511321Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(384, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_name =\"L_16_imagenet1k\"# 'B_16_imagenet1k'\n",
    "\n",
    "model = ViT(model_name, pretrained=True).to(device)\n",
    "model.fc=nn.Identity()\n",
    "\n",
    "model.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T10:56:25.794390Z",
     "start_time": "2021-04-04T10:56:25.790389Z"
    }
   },
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "    def forward(self, input):\n",
    "        return input * torch.tanh(F.softplus(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"model_save/transformer_B_16_imagenet1k_classifier\"+name_mod+\".pth\"\n",
    "            torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO TRY:\n",
    "   - raw input in one or all layer \n",
    "   - every transformer layer\n",
    "   - more complex feature extracter like spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T09:40:25.951838Z",
     "start_time": "2021-04-05T09:40:25.866840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "amp_db=transforms.AmplitudeToDB().to(device)\n",
    "\n",
    "# index = 1\n",
    "   \n",
    "normalize_spec_db, all_spectro, trainloader, testloader=init_param(model)\n",
    "use_wave=False\n",
    "if use_wave==True:\n",
    "\n",
    "    trainset  = Audio_classification(\"train\",train_index,data,label_dic,time_audio_resize=15)\n",
    "    trainloader  = DataLoader(trainset , batch_size=8,\n",
    "                            shuffle=True, num_workers=0)\n",
    "\n",
    "    testset = Audio_classification(\"test\",test_index,data,label_dic,time_audio_resize=15)\n",
    "    testloader   = DataLoader(testset , batch_size=8,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "transform_train_db = T.Compose([  amp_db,     \n",
    "                                \n",
    "        normalize_spec_db,\n",
    "         # transforms.TimeStretch(fixed_rate =1.2),\n",
    "        #transforms.TimeMasking(time_mask_param=20),\n",
    "        #transforms.FrequencyMasking(freq_mask_param=30),\n",
    "        #T.RandomHorizontalFlip(),\n",
    "        #T.Resize((image_size,image_size))\n",
    "                       ])\n",
    "transform_test_db = T.Compose([amp_db,           \n",
    "        normalize_spec_db])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# draft for google cloud computing v100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T11:01:46.887492Z",
     "start_time": "2021-04-05T11:01:46.883492Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_model_pretrained=['B_16', 'B_32', 'L_32', 'B_16_imagenet1k', 'B_32_imagenet1k', 'L_16_imagenet1k', 'L_32_imagenet1k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T09:48:39.056716Z",
     "start_time": "2021-04-05T09:48:37.603768Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_transf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-78ca2c224f20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mnormal_ViT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorrect_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_transf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_spectro\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform_train_db\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_train_db\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUse_waveform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorrect_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_transf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_spectro\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform_test_db\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUse_waveform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mTrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_transf' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "grad_clip = 0.2\n",
    "amp_db=transforms.AmplitudeToDB().to(device)\n",
    "\n",
    "# weight_decay = 1e-6\n",
    "for model_name in all_model_pretrained:\n",
    "    torch.cuda.empty_cache()\n",
    "    #init model\n",
    "    model = ViT(model_name, pretrained=False).to(device)\n",
    "    dim_out=PRETRAINED_MODELS[model_name][\"config\"][\"dim\"]\n",
    "\n",
    "    normalize_spec_db, all_spectro, trainloader, testloader=init_param(model)\n",
    "\n",
    "    transform_train_db = T.Compose([ amp_db,     \n",
    "                                    normalize_spec_db,])\n",
    "\n",
    "    transform_test_db = T.Compose([amp_db,           \n",
    "            normalize_spec_db])\n",
    "\n",
    "    model.fc=nn.Sequential(nn.Linear(dim_out, 1024),nn.ReLU(),nn.Linear(1024, 8)).to(device)\n",
    "    normal_ViT=Normal_ViT(model).to(device)\n",
    "    optimizer = optim.Adam(normal_ViT.parameters(), lr=0.001,weight_decay=0)\n",
    "    #init list\n",
    "    Train_loss=[]\n",
    "    Test_loss=[]\n",
    "    Train_correct=[]\n",
    "    Test_correct=[]\n",
    "    best_model=45\n",
    "    epochs=2\n",
    "    normal_ViT.freeze()\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-5, steps_per_epoch=len(trainloader), epochs=epochs)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if epoch==2:\n",
    "            normal_ViT.unfreeze()\n",
    "        train_loss,correct_train=train_transf(model, device, trainloader, criterion, optimizer, epoch,scheduler,all_spectro,transform_train_db=transform_train_db,log_interval=100,grad_clip=grad_clip,Use_waveform=False)\n",
    "        test_loss,correct_test=test_transf(model, device, testloader,criterion,all_spectro,transform_test_db,Use_waveform=False)\n",
    "        Train_loss.append(train_loss)\n",
    "        Test_loss.append(test_loss)\n",
    "        Train_correct.append(correct_train)\n",
    "        Test_correct.append(correct_test)\n",
    "        print(\"lr = \",optimizer.param_groups[0]['lr'])\n",
    "        if correct_test>best_model:\n",
    "            best_model=correct_test\n",
    "            path=\"model_save/Normal_ViT_\"+model_name+\"_spec.pth\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "    plt.plot(Train_loss)\n",
    "    plt.plot(Test_loss)\n",
    "    plt.legend([\"train\",\"test\"])\n",
    "    plt.title(\"CrossEntropyLoss \"+model_name)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.savefig('loss_'+model_name+' _spectro_all_spectr.png')\n",
    "    plt.show()\n",
    "                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name = 'B_16_imagenet1k' #86859496\n",
    "model_name = \"B_32_imagenet1k\" #88297192\n",
    "model_name = \"B_32_imagenet1k\" #88297192\n",
    "model_name =\"L_16_imagenet1k\" #304 715 752\n",
    "model_name =\"L_32_imagenet1k\" #306 632 680\n",
    "# just Pretrained on ImageNet-21k\n",
    "model_name =\"L_32\" #327 899 475\n",
    "model_name =\"B_16\" #102 595 923\n",
    "model_name =\"B_32\" #104 252 499\n",
    "model_name =\"L_16\" # pretraine param not avaiable\n",
    "model_name =\"L_32\" #327 899 475"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "756.961px",
    "left": "2405.72px",
    "top": "200.352px",
    "width": "267.891px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "319px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
